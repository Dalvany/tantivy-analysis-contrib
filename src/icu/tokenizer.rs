#![feature(test)]
use rust_icu::brk::UBreakIterator;
use std::str::Chars;
use tantivy::tokenizer::{BoxTokenStream, Token, TokenStream, Tokenizer};

/// Default rules, copy from Lucene's binary rules
const DEFAULT_RULES: &str = std::include_str!("breaking_rules/Default.rbbi");
/// Myanmar rules, copy from Lucene's binary rules
const MYANMAR_SYLLABLE_RULES: &str = std::include_str!("breaking_rules/MyanmarSyllable.rbbi");

struct ICUBreakingWord<'a> {
    text: Chars<'a>,
    default_breaking_iterator: UBreakIterator,
}

impl<'a> From<&'a str> for ICUBreakingWord<'a> {
    fn from(text: &'a str) -> Self {
        ICUBreakingWord {
            text: text.chars(),
            default_breaking_iterator: UBreakIterator::try_new_rules(DEFAULT_RULES, text)
                .expect("Can't read default rules."),
        }
    }
}

impl<'a> Iterator for ICUBreakingWord<'a> {
    type Item = (String, usize, usize);

    fn next(&mut self) -> Option<Self::Item> {
        let mut cont = true;
        let mut start = self.default_breaking_iterator.current();
        let mut end = self.default_breaking_iterator.next();
        while cont && end.is_some() {
            if end.is_some() && self.default_breaking_iterator.get_rule_status() == 0 {
                start = end.unwrap();
                end = self.default_breaking_iterator.next();
            }
            if let Some(index) = end {
                cont = !self
                    .text
                    .clone()
                    .take(index as usize)
                    .skip(start as usize)
                    .any(char::is_alphanumeric);
            }
        }

        match end {
            None => Option::None,
            Some(index) => {
                let substring: String = self
                    .text
                    .clone()
                    .take(index as usize)
                    .skip(start as usize)
                    .collect();
                Option::Some((substring, start as usize, index as usize))
            }
        }
    }
}

/// ICU [Tokenizer]. It does not (yet ?) work as Lucene's counterpart.
#[derive(Clone)]
pub struct ICUTokenizer;

impl Tokenizer for ICUTokenizer {
    fn token_stream<'a>(&self, text: &'a str) -> BoxTokenStream<'a> {
        BoxTokenStream::from(ICUTokenizerTokenStream::new(text))
    }
}

/// ICU [TokenStream], it relies on [us::UnicodeWordIndices]
/// to do the actual tokenizing.
pub struct ICUTokenizerTokenStream<'a> {
    breaking_word: ICUBreakingWord<'a>,
    token: Token,
}

impl<'a> ICUTokenizerTokenStream<'a> {
    fn new(text: &'a str) -> Self {
        ICUTokenizerTokenStream {
            breaking_word: ICUBreakingWord::from(text),
            token: Token::default(),
        }
    }
}

impl<'a> TokenStream for ICUTokenizerTokenStream<'a> {
    fn advance(&mut self) -> bool {
        let token = self.breaking_word.next();
        match token {
            None => false,
            Some(token) => {
                self.token.text.clear();
                self.token.position = self.token.position.wrapping_add(1);
                self.token.offset_from = token.1;
                self.token.offset_to = token.2;
                self.token.text.push_str(&token.0);
                true
            }
        }
    }

    fn token(&self) -> &Token {
        &self.token
    }

    fn token_mut(&mut self) -> &mut Token {
        &mut self.token
    }
}

#[cfg(test)]
mod tests {
    /// Same tests as Lucene ICU tokenizer might be enough
    /// TODO non working tests see other unicode library google's rust_icu or icu4x.
    /// TODO run Lucene tests to get offsets so that they can be checked here.
    use super::*;

    impl<'a> Iterator for ICUTokenizerTokenStream<'a> {
        type Item = Token;

        fn next(&mut self) -> Option<Self::Item> {
            if self.advance() {
                return Option::Some(self.token.clone());
            }

            Option::None
        }
    }

    #[test]
    fn test_huge_doc() {
        let mut huge_doc = " ".repeat(4094);
        huge_doc.push_str("testing 1234");
        let tokenizer = &mut ICUTokenizerTokenStream::new(huge_doc.as_str());
        let result: Vec<Token> = tokenizer.collect();
        let expected = vec![
            Token {
                offset_from: 4094,
                offset_to: 4101,
                position: 0,
                text: "testing".to_string(),
                position_length: 1,
            },
            Token {
                offset_from: 4102,
                offset_to: 4106,
                position: 1,
                text: "1234".to_string(),
                position_length: 1,
            },
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_armenian() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« 13 Õ´Õ«Õ¬Õ«Õ¸Õ¶ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ (4,600` Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶ Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´) Õ£Ö€Õ¾Õ¥Õ¬ Õ¥Õ¶ Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ« Õ¯Õ¸Õ²Õ´Õ«Ö Õ¸Ö‚ Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡ Õ¢Õ¸Õ¬Õ¸Ö€ Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬ ÖÕ¡Õ¶Õ¯Õ¡Ö Õ´Õ¡Ö€Õ¤ Õ¸Õ¾ Õ¯Õ¡Ö€Õ¸Õ² Õ§ Õ¢Õ¡ÖÕ¥Õ¬ ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ« Õ¯Õ¡ÕµÖ„Õ¨Ö‰");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«".to_string(),
            "13".to_string(),
            "Õ´Õ«Õ¬Õ«Õ¸Õ¶".to_string(),
            "Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨".to_string(),
            "4,600".to_string(),
            "Õ°Õ¡ÕµÕ¥Ö€Õ¥Õ¶".to_string(),
            "Õ¾Õ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ¸Ö‚Õ´".to_string(),
            "Õ£Ö€Õ¾Õ¥Õ¬".to_string(),
            "Õ¥Õ¶".to_string(),
            "Õ¯Õ¡Õ´Õ¡Õ¾Õ¸Ö€Õ¶Õ¥Ö€Õ«".to_string(),
            "Õ¯Õ¸Õ²Õ´Õ«Ö".to_string(),
            "Õ¸Ö‚".to_string(),
            "Õ°Õ¡Õ´Õ¡Ö€ÕµÕ¡".to_string(),
            "Õ¢Õ¸Õ¬Õ¸Ö€".to_string(),
            "Õ°Õ¸Õ¤Õ¾Õ¡Õ®Õ¶Õ¥Ö€Õ¨".to_string(),
            "Õ¯Õ¡Ö€Õ¸Õ²".to_string(),
            "Õ§".to_string(),
            "Õ­Õ´Õ¢Õ¡Õ£Ö€Õ¥Õ¬".to_string(),
            "ÖÕ¡Õ¶Õ¯Õ¡Ö".to_string(),
            "Õ´Õ¡Ö€Õ¤".to_string(),
            "Õ¸Õ¾".to_string(),
            "Õ¯Õ¡Ö€Õ¸Õ²".to_string(),
            "Õ§".to_string(),
            "Õ¢Õ¡ÖÕ¥Õ¬".to_string(),
            "ÕÕ«Ö„Õ«ÕºÕ¥Õ¤Õ«Õ¡ÕµÕ«".to_string(),
            "Õ¯Õ¡ÕµÖ„Õ¨".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_amharic() {
        let tokenizer = &mut ICUTokenizerTokenStream::new(
            "á‹ŠáŠªá”á‹µá‹« á‹¨á‰£áˆˆ á‰¥á‹™ á‰‹áŠ•á‰‹ á‹¨á‰°áˆŸáˆ‹ á‰µáŠ­áŠ­áˆˆáŠ›áŠ“ áŠáŒ» áˆ˜á‹áŒˆá‰  á‹•á‹á‰€á‰µ (áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«) áŠá‹á¢ áˆ›áŠ•áŠ›á‹áˆ",
        );
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "á‹ŠáŠªá”á‹µá‹«".to_string(),
            "á‹¨á‰£áˆˆ".to_string(),
            "á‰¥á‹™".to_string(),
            "á‰‹áŠ•á‰‹".to_string(),
            "á‹¨á‰°áˆŸáˆ‹".to_string(),
            "á‰µáŠ­áŠ­áˆˆáŠ›áŠ“".to_string(),
            "áŠáŒ»".to_string(),
            "áˆ˜á‹áŒˆá‰ ".to_string(),
            "á‹•á‹á‰€á‰µ".to_string(),
            "áŠ¢áŠ•áˆ³á‹­áŠ­áˆá’á‹²á‹«".to_string(),
            "áŠá‹".to_string(),
            "áˆ›áŠ•áŠ›á‹áˆ".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_arabic() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("Ø§Ù„ÙÙŠÙ„Ù… Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ Ø§Ù„Ø£ÙˆÙ„ Ø¹Ù† ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§ ÙŠØ³Ù…Ù‰ \"Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø© Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…: Ù‚ØµØ© ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§\" (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Truth in Numbers: The Wikipedia Story)ØŒ Ø³ÙŠØªÙ… Ø¥Ø·Ù„Ø§Ù‚Ù‡ ÙÙŠ 2008.");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "Ø§Ù„ÙÙŠÙ„Ù…".to_string(),
            "Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ÙŠ".to_string(),
            "Ø§Ù„Ø£ÙˆÙ„".to_string(),
            "Ø¹Ù†".to_string(),
            "ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§".to_string(),
            "ÙŠØ³Ù…Ù‰".to_string(),
            "Ø§Ù„Ø­Ù‚ÙŠÙ‚Ø©".to_string(),
            "Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù…".to_string(),
            "Ù‚ØµØ©".to_string(),
            "ÙˆÙŠÙƒÙŠØ¨ÙŠØ¯ÙŠØ§".to_string(),
            "Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©".to_string(),
            "Truth".to_string(),
            "in".to_string(),
            "Numbers".to_string(),
            "The".to_string(),
            "Wikipedia".to_string(),
            "Story".to_string(),
            "Ø³ÙŠØªÙ…".to_string(),
            "Ø¥Ø·Ù„Ø§Ù‚Ù‡".to_string(),
            "ÙÙŠ".to_string(),
            "2008".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_aramaic() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ (ÜÜ¢Ü“Ü ÜÜ: Wikipedia) Ü—Ü˜ ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ ÜšÜÜªÜ¬Ü Ü•ÜÜ¢Ü›ÜªÜ¢Ü› Ü’Ü Ü«Ü¢ÌˆÜ Ü£Ü“ÜÜÌˆÜÜ‚ Ü«Ü¡Ü— ÜÜ¬Ü Ü¡Ü¢ Ü¡ÌˆÜ Ü¬Ü Ü•\"Ü˜ÜÜ©Ü\" Ü˜\"ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ\"Ü€");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "Ü˜ÜÜ©ÜÜ¦Ü•ÜÜ".to_string(),
            "ÜÜ¢Ü“Ü ÜÜ".to_string(),
            "Wikipedia".to_string(),
            "Ü—Ü˜".to_string(),
            "ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ".to_string(),
            "ÜšÜÜªÜ¬Ü".to_string(),
            "Ü•ÜÜ¢Ü›ÜªÜ¢Ü›".to_string(),
            "Ü’Ü Ü«Ü¢ÌˆÜ".to_string(),
            "Ü£Ü“ÜÜÌˆÜ".to_string(),
            "Ü«Ü¡Ü—".to_string(),
            "ÜÜ¬Ü".to_string(),
            "Ü¡Ü¢".to_string(),
            "Ü¡ÌˆÜ Ü¬Ü".to_string(),
            "Ü•".to_string(),
            "Ü˜ÜÜ©Ü".to_string(),
            "Ü˜".to_string(),
            "ÜÜÜ¢Ü£Ü©Ü Ü˜Ü¦Ü•ÜÜ".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_bengali() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("à¦à¦‡ à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦· à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾ à¦•à¦°à§‡ à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨ (à¦à¦•à¦Ÿà¦¿ à¦…à¦²à¦¾à¦­à¦œà¦¨à¦• à¦¸à¦‚à¦¸à§à¦¥à¦¾)à¥¤ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦° à¦¶à§à¦°à§ à§§à§« à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿, à§¨à§¦à§¦à§§ à¦¸à¦¾à¦²à§‡à¥¤ à¦à¦–à¦¨ à¦ªà¦°à§à¦¯à¦¨à§à¦¤ à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“ à¦¬à§‡à¦¶à§€ à¦­à¦¾à¦·à¦¾à¦¯à¦¼ à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾ à¦°à¦¯à¦¼à§‡à¦›à§‡à¥¤");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "à¦à¦‡".to_string(),
            "à¦¬à¦¿à¦¶à§à¦¬à¦•à§‹à¦·".to_string(),
            "à¦ªà¦°à¦¿à¦šà¦¾à¦²à¦¨à¦¾".to_string(),
            "à¦•à¦°à§‡".to_string(),
            "à¦‰à¦‡à¦•à¦¿à¦®à¦¿à¦¡à¦¿à¦¯à¦¼à¦¾".to_string(),
            "à¦«à¦¾à¦‰à¦¨à§à¦¡à§‡à¦¶à¦¨".to_string(),
            "à¦à¦•à¦Ÿà¦¿".to_string(),
            "à¦…à¦²à¦¾à¦­à¦œà¦¨à¦•".to_string(),
            "à¦¸à¦‚à¦¸à§à¦¥à¦¾".to_string(),
            "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾à¦°".to_string(),
            "à¦¶à§à¦°à§".to_string(),
            "à§§à§«".to_string(),
            "à¦œà¦¾à¦¨à§à¦¯à¦¼à¦¾à¦°à¦¿".to_string(),
            "à§¨à§¦à§¦à§§".to_string(),
            "à¦¸à¦¾à¦²à§‡".to_string(),
            "à¦à¦–à¦¨".to_string(),
            "à¦ªà¦°à§à¦¯à¦¨à§à¦¤".to_string(),
            "à§¨à§¦à§¦à¦Ÿà¦¿à¦°à¦“".to_string(),
            "à¦¬à§‡à¦¶à§€".to_string(),
            "à¦­à¦¾à¦·à¦¾à¦¯à¦¼".to_string(),
            "à¦‰à¦‡à¦•à¦¿à¦ªà¦¿à¦¡à¦¿à¦¯à¦¼à¦¾".to_string(),
            "à¦°à¦¯à¦¼à§‡à¦›à§‡".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_farsi() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ÙˆÛŒÚ©ÛŒ Ù¾Ø¯ÛŒØ§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± ØªØ§Ø±ÛŒØ® Û²Ûµ Ø¯ÛŒ Û±Û³Û·Û¹ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ú©Ù…Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù” ØªØ®ØµØµÛŒ Ù†ÙˆÙ¾Ø¯ÛŒØ§ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯.");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "ÙˆÛŒÚ©ÛŒ".to_string(),
            "Ù¾Ø¯ÛŒØ§ÛŒ".to_string(),
            "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ".to_string(),
            "Ø¯Ø±".to_string(),
            "ØªØ§Ø±ÛŒØ®".to_string(),
            "Û²Ûµ".to_string(),
            "Ø¯ÛŒ".to_string(),
            "Û±Û³Û·Û¹".to_string(),
            "Ø¨Ù‡".to_string(),
            "ØµÙˆØ±Øª".to_string(),
            "Ù…Ú©Ù…Ù„ÛŒ".to_string(),
            "Ø¨Ø±Ø§ÛŒ".to_string(),
            "Ø¯Ø§Ù†Ø´Ù†Ø§Ù…Ù‡Ù”".to_string(),
            "ØªØ®ØµØµÛŒ".to_string(),
            "Ù†ÙˆÙ¾Ø¯ÛŒØ§".to_string(),
            "Ù†ÙˆØ´ØªÙ‡".to_string(),
            "Ø´Ø¯".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_greek() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹ ÏƒÎµ ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î± Î±Ï€ÏŒ ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚ Î¼Îµ Ï„Î¿ Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ wiki, ÎºÎ¬Ï„Î¹ Ï€Î¿Ï… ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹ ÏŒÏ„Î¹ Î¬ÏÎ¸ÏÎ± Î¼Ï€Î¿ÏÎµÎ¯ Î½Î± Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½ Î® Î½Î± Î±Î»Î»Î¬Î¾Î¿Ï…Î½ Î±Ï€ÏŒ Ï„Î¿Î½ ÎºÎ±Î¸Î­Î½Î±.");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "Î“ÏÎ¬Ï†ÎµÏ„Î±Î¹".to_string(),
            "ÏƒÎµ".to_string(),
            "ÏƒÏ…Î½ÎµÏÎ³Î±ÏƒÎ¯Î±".to_string(),
            "Î±Ï€ÏŒ".to_string(),
            "ÎµÎ¸ÎµÎ»Î¿Î½Ï„Î­Ï‚".to_string(),
            "Î¼Îµ".to_string(),
            "Ï„Î¿".to_string(),
            "Î»Î¿Î³Î¹ÏƒÎ¼Î¹ÎºÏŒ".to_string(),
            "wiki".to_string(),
            "ÎºÎ¬Ï„Î¹".to_string(),
            "Ï€Î¿Ï…".to_string(),
            "ÏƒÎ·Î¼Î±Î¯Î½ÎµÎ¹".to_string(),
            "ÏŒÏ„Î¹".to_string(),
            "Î¬ÏÎ¸ÏÎ±".to_string(),
            "Î¼Ï€Î¿ÏÎµÎ¯".to_string(),
            "Î½Î±".to_string(),
            "Ï€ÏÎ¿ÏƒÏ„ÎµÎ¸Î¿ÏÎ½".to_string(),
            "Î®".to_string(),
            "Î½Î±".to_string(),
            "Î±Î»Î»Î¬Î¾Î¿Ï…Î½".to_string(),
            "Î±Ï€ÏŒ".to_string(),
            "Ï„Î¿Î½".to_string(),
            "ÎºÎ±Î¸Î­Î½Î±".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_khmer() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("á•áŸ’á‘áŸ‡áŸáŸ’á€á¹á˜áŸáŸ’á€áŸƒá”á¸á”á½á“ááŸ’á“á„á“áŸáŸ‡");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "á•áŸ’á‘áŸ‡".to_string(),
            "áŸáŸ’á€á¹á˜áŸáŸ’á€áŸƒ".to_string(),
            "á”á¸".to_string(),
            "á”á½á“".to_string(),
            "ááŸ’á“á„".to_string(),
            "á“áŸáŸ‡".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_lao() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("àºàº§à»ˆàº²àº”àº­àº");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["àºàº§à»ˆàº²".to_string(), "àº”àº­àº".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("àºàº²àºªàº²àº¥àº²àº§");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["àºàº²àºªàº²".to_string(), "àº¥àº²àº§".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_myanmar() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("á€á€€á€ºá€á€„á€ºá€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸á€…á€±á€•á€¼á€®á€¸");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "á€á€€á€ºá€á€„á€º".to_string(),
            "á€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸".to_string(),
            "á€…á€±".to_string(),
            "á€•á€¼á€®á€¸".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_thai() {
        let tokenizer =
            &mut ICUTokenizerTokenStream::new("à¸à¸²à¸£à¸—à¸µà¹ˆà¹„à¸”à¹‰à¸•à¹‰à¸­à¸‡à¹à¸ªà¸”à¸‡à¸§à¹ˆà¸²à¸‡à¸²à¸™à¸”à¸µ. à¹à¸¥à¹‰à¸§à¹€à¸˜à¸­à¸ˆà¸°à¹„à¸›à¹„à¸«à¸™? à¹‘à¹’à¹“à¹”");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "à¸à¸²à¸£".to_string(),
            "à¸—à¸µà¹ˆ".to_string(),
            "à¹„à¸”à¹‰".to_string(),
            "à¸•à¹‰à¸­à¸‡".to_string(),
            "à¹à¸ªà¸”à¸‡".to_string(),
            "à¸§à¹ˆà¸²".to_string(),
            "à¸‡à¸²à¸™".to_string(),
            "à¸”à¸µ".to_string(),
            "à¹à¸¥à¹‰à¸§".to_string(),
            "à¹€à¸˜à¸­".to_string(),
            "à¸ˆà¸°".to_string(),
            "à¹„à¸›".to_string(),
            "à¹„à¸«à¸™".to_string(),
            "à¹‘à¹’à¹“à¹”".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_tibetan() {
        let tokenizer = &mut ICUTokenizerTokenStream::new(
            "à½¦à¾£à½¼à½“à¼‹à½˜à½›à½¼à½‘à¼‹à½‘à½„à¼‹à½£à½¦à¼‹à½ à½‘à½²à½¦à¼‹à½–à½¼à½‘à¼‹à½¡à½²à½‚à¼‹à½˜à½²à¼‹à½‰à½˜à½¦à¼‹à½‚à½¼à½„à¼‹à½ à½•à½ºà½£à¼‹à½‘à½´à¼‹à½‚à½à½¼à½„à¼‹à½–à½¢à¼‹à½§à¼‹à½…à½„à¼‹à½‘à½‚à½ºà¼‹à½˜à½šà½“à¼‹à½˜à½†à½²à½¦à¼‹à½¦à½¼à¼ à¼",
        );
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "à½¦à¾£à½¼à½“".to_string(),
            "à½˜à½›à½¼à½‘".to_string(),
            "à½‘à½„".to_string(),
            "à½£à½¦".to_string(),
            "à½ à½‘à½²à½¦".to_string(),
            "à½–à½¼à½‘".to_string(),
            "à½¡à½²à½‚".to_string(),
            "à½˜à½²".to_string(),
            "à½‰à½˜à½¦".to_string(),
            "à½‚à½¼à½„".to_string(),
            "à½ à½•à½ºà½£".to_string(),
            "à½‘à½´".to_string(),
            "à½‚à½à½¼à½„".to_string(),
            "à½–à½¢".to_string(),
            "à½§".to_string(),
            "à½…à½„".to_string(),
            "à½‘à½‚à½º".to_string(),
            "à½˜à½šà½“".to_string(),
            "à½˜à½†à½²à½¦".to_string(),
            "à½¦à½¼".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_chinese() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("æˆ‘æ˜¯ä¸­å›½äººã€‚ ï¼‘ï¼’ï¼“ï¼” ï¼´ï½…ï½“ï½”ï½“ ");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "æˆ‘".to_string(),
            "æ˜¯".to_string(),
            "ä¸­".to_string(),
            "å›½".to_string(),
            "äºº".to_string(),
            "ï¼‘ï¼’ï¼“ï¼”".to_string(),
            "ï¼´ï½…ï½“ï½”ï½“".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_hebrew() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("×“× ×§× ×¨ ×ª×§×£ ××ª ×”×“×•\"×—");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "×“× ×§× ×¨".to_string(),
            "×ª×§×£".to_string(),
            "××ª".to_string(),
            "×”×“×•\"×—".to_string(),
        ];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("×—×‘×¨×ª ×‘×ª ×©×œ ××•×“×™'×¡");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "×—×‘×¨×ª".to_string(),
            "×‘×ª".to_string(),
            "×©×œ".to_string(),
            "××•×“×™'×¡".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_empty() {
        let expected: Vec<String> = vec![];

        let tokenizer = &mut ICUTokenizerTokenStream::new("");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new(".");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new(" ");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        assert_eq!(result, expected);
    }

    #[test]
    fn test_lucene1545() {
        /*
         * Standard analyzer does not correctly tokenize combining character U+0364 COMBINING LATIN SMALL LETTRE E.
         * The word "moÍ¤chte" is incorrectly tokenized into "mo" "chte", the combining character is lost.
         * Expected result is only on token "moÍ¤chte".
         */
        let tokenizer = &mut ICUTokenizerTokenStream::new("moÍ¤chte");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["moÍ¤chte".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_alphanumeric_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("B2B");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["B2B".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("2B");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["2B".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_delimiters_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("some-dashed-phrase");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "some".to_string(),
            "dashed".to_string(),
            "phrase".to_string(),
        ];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("dogs,chase,cats");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["dogs".to_string(), "chase".to_string(), "cats".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("ac/dc");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ac".to_string(), "dc".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_apostrophes_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("O'Reilly");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["O'Reilly".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("you're");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["you're".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("she's");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["she's".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("Jim's");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["Jim's".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("don't");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["don't".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("O'Reilly's");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["O'Reilly's".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_numeric_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("21.35");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["21.35".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("R2D2 C3PO");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["R2D2".to_string(), "C3PO".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("R2D2 C3PO");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["R2D2".to_string(), "C3PO".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("216.239.63.104");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["216.239.63.104".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_text_with_numbers_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("David has 5000 bones");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "David".to_string(),
            "has".to_string(),
            "5000".to_string(),
            "bones".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_various_text_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("C embedded developers wanted");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "C".to_string(),
            "embedded".to_string(),
            "developers".to_string(),
            "wanted".to_string(),
        ];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("foo bar FOO BAR");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "foo".to_string(),
            "bar".to_string(),
            "FOO".to_string(),
            "BAR".to_string(),
        ];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("foo      bar .  FOO <> BAR");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "foo".to_string(),
            "bar".to_string(),
            "FOO".to_string(),
            "BAR".to_string(),
        ];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("\"QUOTED\" word");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["QUOTED".to_string(), "word".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_korean_sa() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ì•ˆë…•í•˜ì„¸ìš” í•œê¸€ì…ë‹ˆë‹¤");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ì•ˆë…•í•˜ì„¸ìš”".to_string(), "í•œê¸€ì…ë‹ˆë‹¤".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_offsets() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("David has 5000 bones");
        let result: Vec<Token> = tokenizer.collect();
        let expected = vec![
            Token {
                offset_from: 0,
                offset_to: 5,
                position: 0,
                text: "David".to_string(),
                position_length: 1,
            },
            Token {
                offset_from: 6,
                offset_to: 9,
                position: 1,
                text: "has".to_string(),
                position_length: 1,
            },
            Token {
                offset_from: 10,
                offset_to: 14,
                position: 2,
                text: "5000".to_string(),
                position_length: 1,
            },
            Token {
                offset_from: 15,
                offset_to: 20,
                position: 3,
                text: "bones".to_string(),
                position_length: 1,
            },
        ];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_korean() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("í›ˆë¯¼ì •ìŒ");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["í›ˆë¯¼ì •ìŒ".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    fn test_japanese() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ä»®åé£ã„ ã‚«ã‚¿ã‚«ãƒŠ");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "ä»®".to_string(),
            "å".to_string(),
            "é£".to_string(),
            "ã„".to_string(),
            "ã‚«ã‚¿ã‚«ãƒŠ".to_string(),
        ];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ğŸ’© ğŸ’©ğŸ’©");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ğŸ’©".to_string(), "ğŸ’©".to_string(), "ğŸ’©".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji_sequence() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ğŸ‘©â€â¤ï¸â€ğŸ‘©");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ğŸ‘©â€â¤ï¸â€ğŸ‘©".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji_sequence_with_modifier() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ğŸ‘¨ğŸ¼â€âš•ï¸");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ğŸ‘¨ğŸ¼â€âš•ï¸".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji_regional_indicator() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ğŸ‡ºğŸ‡¸ğŸ‡ºğŸ‡¸");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ğŸ‡ºğŸ‡¸".to_string(), "ğŸ‡ºğŸ‡¸".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji_variation_sequence() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("#ï¸âƒ£");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["3ï¸âƒ£".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji_tag_sequence() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["ğŸ´ó §ó ¢ó ¥ó ®ó §ó ¿".to_string()];
        assert_eq!(result, expected);
    }

    #[test]
    #[ignore]
    fn test_emoji_tokenization() {
        let tokenizer = &mut ICUTokenizerTokenStream::new("pooğŸ’©poo");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec!["poo".to_string(), "ğŸ’©".to_string(), "poo".to_string()];
        assert_eq!(result, expected);

        let tokenizer = &mut ICUTokenizerTokenStream::new("ğŸ’©ä¸­åœ‹ğŸ’©");
        let result: Vec<String> = tokenizer.map(|v| v.text).collect();
        let expected = vec![
            "ğŸ’©".to_string(),
            "ä¸­".to_string(),
            "åœ‹".to_string(),
            "ğŸ’©".to_string(),
        ];
        assert_eq!(result, expected);
    }
}
